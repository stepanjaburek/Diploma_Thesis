# -*- coding: utf-8 -*-
"""EU_Machine_Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzpaP7VLeYjWoYc3YzqhZG-d7Y22CJE3

<a href="https://colab.research.google.com/github/stepanjaburek/workingpaper_czech_psp_speeches/blob/main/streamline_translation_sentiment.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# **Machine translation**

# Setup
"""

!pip install transformers sentencepiece sacremoses torch tqdm

import pandas as pd
from transformers import MarianMTModel, MarianTokenizer
from tqdm.notebook import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

"""# **NLLB-200 model from Meta**"""

def translate_csv_nllb(file_path, source_lang='ces_Latn', target_lang='eng_Latn', batch_size=16, max_length=256):

    df = pd.read_csv(file_path)

    model_name = 'facebook/nllb-200-distilled-600M'
    tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=source_lang)


    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.float16 if device == 'cuda' else torch.float32

    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        torch_dtype=dtype,
        device_map=device
    )
    target_token_id = tokenizer.convert_tokens_to_ids(target_lang)

    if device == 'cuda':
        torch.cuda.empty_cache()

    # Process in batches more efficiently
    translated_texts = []

    # Use a larger batch size on powerful hardware
    effective_batch_size = batch_size
    if device == 'cuda':
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # in GB
            if gpu_memory > 20:
                effective_batch_size = min(32, batch_size)
        except:
            # In case we can't get GPU properties
            pass

    # Use tqdm for progress tracking
    for i in tqdm(range(0, len(df), effective_batch_size), desc="Translating batches"):
        batch_texts = df['context_full'][i:i + effective_batch_size].tolist()

        # Skip empty texts
        batch_texts = [text for text in batch_texts if isinstance(text, str) and text.strip()]
        if not batch_texts:
            continue

        # Tokenize
        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(device)

        # Generate with optimized parameters
        with torch.no_grad():  # Disable gradient calculation for inference
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=target_token_id,
                max_length=max_length,
                num_beams=4,  # Beam search for better quality
                length_penalty=1.0,
                early_stopping=True
            )

        # Decode the outputs
        batch_translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        translated_texts.extend(batch_translations)

        # Clean up memory
        if device == 'cuda':
            del inputs, outputs
            torch.cuda.empty_cache()

    # Add translations to the dataframe
    df['translated_context_full'] = translated_texts

    return df

translated_df = translate_csv_nllb('/content/eu.csv')
translated_df.to_csv('eu_translated.csv', index=False)

translated_df = translate_csv_nllb('/content/eu_institutions.csv')
translated_df.to_csv('eu_institutions_translated.csv', index=False)