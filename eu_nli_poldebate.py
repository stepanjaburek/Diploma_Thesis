# -*- coding: utf-8 -*-
"""EU_NLI_PolDEBATE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eU_Yc-G3GqztoHUHs9QjIToTl13Pw6TY

<a href="https://colab.research.google.com/github/stepanjaburek/workingpaper_czech_psp_speeches/blob/main/streamline_translation_sentiment.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Setup
"""

!pip install transformers datasets

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from tqdm.notebook import tqdm
import torch

"""# **Sentiment classification using the Political DEBATE model by Burnham et al. (2024)**

# Setup
"""

def analyze_sentiments(df, classifier, classes, hypothesis, batch_size=16):
    results = []
    for i in tqdm(range(0, len(df), batch_size)):
        batch_output = classifier(
            df['translated_context_full'][i:i + batch_size].tolist(),
            classes,
            hypothesis_template=hypothesis,
            multi_label=True,
            batch_size=batch_size
        )

        for item in batch_output:
            results.append({
                'label': item['labels'][0],
                'score': item['scores'][0],
                **{f'{label}_score': score for label, score in zip(item['labels'], item['scores'])}
            })

    return pd.DataFrame(results)

"""# Model specification"""

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template =  "The author of this text {} the European Union"
classes = [ "supports", "opposes", "is neutral towards"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/eu_translated.csv")
#sampled_df = df.sample(n=100, random_state=42)
results = analyze_sentiments(df, classifier, classes, hypothesis_template)

pd.concat([df, results], axis=1).to_csv('debate_stance_eu.csv', index=False)


print("\nStance Distribution:")
print(results['label'].value_counts())

"""# Model specification and labeling - framing"""

hypothesis_dict = {
    "Appropriateness": [
        "The speaker is concerned with identities.",
        "The argument is based on shared norms or values.",
        "The speaker focuses on what is morally appropriate.",
        "The speaker appeals to appropriate behavior."

    ],
    "Consequences": [
        "The speaker is concerned with costs and benefits.",
        "The speaker focuses on consequences.",
        "This argument is based on utility.",
        "The speaker appeals to results or effectiveness."
    ]
}

def analyze_sentiments_multi_hypothesis(df, classifier, hypothesis_dict, batch_size=16, aggregation="max"):
    import numpy as np
    results = []

    texts = df['translated_context_full'].tolist()
    num_samples = len(texts)

    for i in tqdm(range(0, num_samples, batch_size)):
        batch_texts = texts[i:i + batch_size]

        # Collect all hypotheses for the batch
        all_hypotheses = []
        for label, hyps in hypothesis_dict.items():
            all_hypotheses.extend(hyps)

        # Run multi-label zero-shot with all hypotheses at once
        batch_output = classifier(
            batch_texts,
            candidate_labels=all_hypotheses,
            hypothesis_template="{}",
            multi_label=True,
            batch_size=batch_size
        )

        for item in batch_output:
            # Create per-label aggregated scores
            aggregated_scores = {}
            all_label_scores = dict(zip(item["labels"], item["scores"]))

            for label, hyps in hypothesis_dict.items():
                scores = [all_label_scores.get(hyp, 0.0) for hyp in hyps]
                if aggregation == "max":
                    aggregated_scores[label] = max(scores)
                else:
                    aggregated_scores[label] = sum(scores) / len(scores)

            # Determine final label
            app = aggregated_scores["Appropriateness"]
            con = aggregated_scores["Consequences"]

            if app < 0.5 and con < 0.5:
                final_label = "Neither"
            elif app >= 0.5 and con >= 0.5:
                final_label = "Both"
            elif app > con:
                final_label = "Appropriateness"
            else:
                final_label = "Consequences"

            results.append({
                "Appropriateness_score": app,
                "Consequences_score": con,
                "label": final_label
            })

    return pd.DataFrame(results)

df = pd.read_csv("/content/eu_translated.csv")
#df = pd.read_csv("/content/eu_1000.csv")

model_name = "mlburnham/Political_DEBATE_large_v1.0"

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)
results = analyze_sentiments_multi_hypothesis(
    df,
    classifier,
    hypothesis_dict,
    batch_size=16,
    aggregation="max"
)

pd.concat([df, results], axis=1).to_csv("debate_discourse_eu.csv", index=False)
print("\nFraming Distribution:")
print(results["label"].value_counts())